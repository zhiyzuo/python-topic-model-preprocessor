{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last updated on 01-26-2018.\n",
    "\n",
    "Written by Zhiya Zuo\n",
    "\n",
    "Email: [zhiyazuo@gmail.com](mailto:zhiyazuo@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of `Preprocessor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:55.527920Z",
     "start_time": "2018-01-26T18:30:53.858745Z"
    }
   },
   "outputs": [],
   "source": [
    "import tm_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:55.553245Z",
     "start_time": "2018-01-26T18:30:55.530023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_preprocessor.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from https://radimrehurek.com/gensim/tut1.html#corpus-formats, with some minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:55.560984Z",
     "start_time": "2018-01-26T18:30:55.555549Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time 2017 [\\t]\",\n",
    "             \"He went to the gym and swam.\",  \n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees, and well quasi ordering\",\n",
    "             \"Graph minors: A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a `Preprocessor` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:55.566622Z",
     "start_time": "2018-01-26T18:30:55.562734Z"
    }
   },
   "outputs": [],
   "source": [
    "from tm_preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:57.014134Z",
     "start_time": "2018-01-26T18:30:57.005796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Preprocessor in module tm_preprocessor.preprocessor:\n",
      "\n",
      "class Preprocessor(builtins.object)\n",
      " |  Preprocessor of text corpus before feeding into topic modeling algorithms\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  corpus : np.array\n",
      " |      Processed corpus.\n",
      " |  documents : iteratble object (list/tuple/numpy array...)\n",
      " |      A list of documents\n",
      " |  punctuations : str\n",
      " |      String sequence of punctuations to be removed. By default: \"!@#$%^*(),.:;&=+-_?'`\\\n",
      " |  stopwords : np.array\n",
      " |      An array of stopwords.\n",
      " |  vocabulary : corpora.Dictionary\n",
      " |      Dictionary of self.corpus\n",
      " |  \n",
      " |  Methods\n",
      " |  -------\n",
      " |  add_stopwords(additional_stopwords)\n",
      " |      Add additional stop words (`additional_stopwords`) to `self.stopwords`.\n",
      " |  tokenize(normalizer, min_freq, max_freq, min_length)\n",
      " |      Tokenize the corpus into bag of words using the specified `stemmer` or `lemmatizer` and\n",
      " |      minimum/maximum frequency (`min_freq` and `max_freq`) and length (`min_length`) of words/tokens.\n",
      " |  serialize(path, format_)\n",
      " |      Serialize corpus in `format_` and build vocabulary. Save dump files to `path`.\n",
      " |  get_word_ranking()\n",
      " |      Obtain frequency rankings of each word/token. Return `None` if none of the preprocessing has been done.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, documents, punctuations='\"!@#$%^*(),.:;&=+-_?\\\\\\'`[]', stopword_file=None)\n",
      " |      Init function for `Preprocessor` class\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iteratble object (list/tuple/numpy array...; init to None)\n",
      " |          Processed collection of documents.\n",
      " |      documents : iteratble object (list/tuple/numpy array...)\n",
      " |          A list of documents. This should not be altered throughout the processing\n",
      " |      punctuations : str\n",
      " |          String sequence of punctuations to be removed. By default: \"[!@#$%^*(),.:;&=+-_?'`\\\n",
      " |      stopword_file : str\n",
      " |          File path for stop word list. By default use pre-defined stopwords.\n",
      " |  \n",
      " |  add_stopwords(self, additional_stopwords)\n",
      " |      Add additional stopwords to the current `Preprocessor` object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      additional_stopwords : iteratble object (list/tuple/numpy array...; init to None)\n",
      " |          Additional stopwords.\n",
      " |  \n",
      " |  get_word_ranking(self)\n",
      " |      Get the ranking of words (tokens). Note that this should be done a\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pd.DataFrame\n",
      " |          Sorted dataframe with columns `word` and corresponding `frequency`.\n",
      " |  \n",
      " |  normalize(self, normalizer, min_freq=0.05, max_freq=0.95, min_length=1)\n",
      " |      Normalize corpus by either lemmatization or stemming. Also remove rare/common and short words\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      stemmer : nltk.stem stemmers or lemmatizers\n",
      " |          Stemmer/lemmatizer to use. See http://www.nltk.org/api/nltk.stem.html. If `None`, do not stem.\n",
      " |      min_freq : float\n",
      " |          The minimum frequency (in ratio) of a token to be kept\n",
      " |      max_freq : float\n",
      " |          The maximum frequency (in ratio) of a token to be kept\n",
      " |      min_length : int\n",
      " |          The minimum length of a token to be kept\n",
      " |  \n",
      " |  remove_digits_punctuactions(self)\n",
      " |  \n",
      " |  remove_stopwords(self)\n",
      " |  \n",
      " |  serialize(self, path='.', format_='MmCorpus')\n",
      " |      Serialize corpus and build vocabulary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          The path to save corpus and vocabulary (current directory by default).\n",
      " |      format_ : str\n",
      " |          The format of the serialized corpus. See https://radimrehurek.com/gensim/tut1.html#corpus-formats\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:30:57.291252Z",
     "start_time": "2018-01-26T18:30:57.278403Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "##### States of the object `preprocessor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### `documents` that store the original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:44.190398Z",
     "start_time": "2018-01-26T18:06:44.181741Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for lab abc computer applications',\n",
       " 'A survey of user opinion of computer system response time 2017 [\\\\t]',\n",
       " 'He went to the gym and swam.',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user perceived response time to error measurement',\n",
       " 'The generation of random binary unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV Widths of trees, and well quasi ordering',\n",
       " 'Graph minors: A survey']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### `punctuations` that are to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Here shows the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:44.773183Z",
     "start_time": "2018-01-26T18:06:44.769335Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"!@#$%^*(),.:;&=+-_?\\\\\\'`[]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### `stopwords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Here shows the default values. Note that one can add more using `preprocessor.add_stopwords` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:45.455411Z",
     "start_time": "2018-01-26T18:06:45.449517Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', \"a's\", 'ableabout', 'about', 'above', 'according',\n",
       "       'accordingly', 'across', 'actually', 'after', 'afterwards', 'again',\n",
       "       'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone',\n",
       "       'along', 'already', 'also', 'although', 'always', 'am', 'among',\n",
       "       'amongst', 'an', 'and', 'and/or', 'another', 'any', 'anybody',\n",
       "       'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere',\n",
       "       'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\",\n",
       "       'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at',\n",
       "       'available', 'away', 'awfully', 'be', 'became', 'because', 'become',\n",
       "       'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind',\n",
       "       'being', 'believe', 'below', 'beside', 'besides', 'best', 'better',\n",
       "       'between', 'beyond', 'both', 'brief', 'bring', 'but', 'by', \"c'mon\",\n",
       "       \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes',\n",
       "       'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come',\n",
       "       'comes', 'concerning', 'consequently', 'consider', 'considering',\n",
       "       'contain', 'containing', 'contains', 'corresponding', 'could',\n",
       "       \"couldn't\", 'course', 'currently', 'definitely', 'described',\n",
       "       'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\",\n",
       "       'doing', 'don', \"don't\", 'done', 'down', 'downwards', 'during',\n",
       "       'each', 'edu', 'eg', 'eight', 'eighteen', 'eighty', 'either',\n",
       "       'eleven', 'else', 'elsewhere', 'enough', 'entirely', 'especially',\n",
       "       'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone',\n",
       "       'everything', 'everywhere', 'ex', 'exactly', 'example', 'except',\n",
       "       'far', 'few', 'fifteen', 'fifth', 'fifty', 'first', 'five',\n",
       "       'followed', 'following', 'follows', 'for', 'former', 'formerly',\n",
       "       'forth', 'forty', 'four', 'fourteen', 'from', 'further',\n",
       "       'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'go',\n",
       "       'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had',\n",
       "       \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\",\n",
       "       'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here',\n",
       "       \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers',\n",
       "       'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully',\n",
       "       'how', 'howbeit', 'however', 'hundred', 'i', \"i'd\", \"i'll\", \"i'm\",\n",
       "       \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc',\n",
       "       'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar',\n",
       "       'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\",\n",
       "       \"it's\", 'its', 'itself', 'just', 'keep', 'keeps', 'kept', 'know',\n",
       "       'known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly',\n",
       "       'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely',\n",
       "       'little', 'look', 'looking', 'looks', 'ltd', 'mainly', 'many',\n",
       "       'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might',\n",
       "       'million', 'more', 'moreover', 'most', 'mostly', 'much', 'must',\n",
       "       'my', 'myself', 'name', 'namely', 'nd', 'near', 'nearly',\n",
       "       'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless',\n",
       "       'new', 'next', 'nine', 'nineteen', 'ninety', 'no', 'nobody', 'non',\n",
       "       'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel',\n",
       "       'now', 'nowhere', 'obviously', 'of', 'off', 'often', 'oh', 'ok',\n",
       "       'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or',\n",
       "       'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves',\n",
       "       'out', 'outside', 'over', 'overall', 'own', 'particular',\n",
       "       'particularly', 'per', 'perhaps', 'placed', 'please', 'plus',\n",
       "       'possible', 'presumably', 'probably', 'provides', 'que', 'quite',\n",
       "       'qv', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding',\n",
       "       'regardless', 'regards', 'relatively', 'respectively', 'right', 's',\n",
       "       'said', 'same', 'saw', 'say', 'saying', 'says', 'second',\n",
       "       'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems',\n",
       "       'seen', 'self', 'selves', 'sensible', 'sent', 'serious',\n",
       "       'seriously', 'seven', 'seventeen', 'seventy', 'several', 'shall',\n",
       "       'she', 'should', \"shouldn't\", 'since', 'six', 'sixteen', 'sixty',\n",
       "       'so', 'some', 'somebody', 'somehow', 'someone', 'something',\n",
       "       'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry',\n",
       "       'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup',\n",
       "       'sure', 't', \"t's\", 'take', 'taken', 'tell', 'ten', 'tends', 'th',\n",
       "       'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats',\n",
       "       'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence',\n",
       "       'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein',\n",
       "       'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\",\n",
       "       \"they're\", \"they've\", 'think', 'third', 'thirteen', 'thirty',\n",
       "       'this', 'thorough', 'thoroughly', 'those', 'though', 'three',\n",
       "       'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too',\n",
       "       'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try',\n",
       "       'trying', 'twelve', 'twenty', 'twice', 'two', 'un', 'under',\n",
       "       'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up',\n",
       "       'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually',\n",
       "       'value', 'various', 'very', 'via', 'viz', 'vs', 'want', 'wants',\n",
       "       'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
       "       'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\",\n",
       "       'whatever', 'when', 'whence', 'whenever', 'where', \"where's\",\n",
       "       'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon',\n",
       "       'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\",\n",
       "       'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing',\n",
       "       'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would',\n",
       "       \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\",\n",
       "       \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'zero'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### `corpus` and `vocabulary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "These are `None` for vocabulary at the moment because no processing has been done at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:46.278658Z",
     "start_time": "2018-01-26T18:06:46.273921Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list(['human', 'machine', 'interface', 'for', 'lab', 'abc', 'computer', 'applications']),\n",
       "       list(['a', 'survey', 'of', 'user', 'opinion', 'of', 'computer', 'system', 'response', 'time', '2017', '[', '\\\\t', ']']),\n",
       "       list(['he', 'went', 'to', 'the', 'gym', 'and', 'swam', '.']),\n",
       "       list(['the', 'eps', 'user', 'interface', 'management', 'system']),\n",
       "       list(['system', 'and', 'human', 'system', 'engineering', 'testing', 'of', 'eps']),\n",
       "       list(['relation', 'of', 'user', 'perceived', 'response', 'time', 'to', 'error', 'measurement']),\n",
       "       list(['the', 'generation', 'of', 'random', 'binary', 'unordered', 'trees']),\n",
       "       list(['the', 'intersection', 'graph', 'of', 'paths', 'in', 'trees']),\n",
       "       list(['graph', 'minors', 'iv', 'widths', 'of', 'trees', ',', 'and', 'well', 'quasi', 'ordering']),\n",
       "       list(['graph', 'minors', ':', 'a', 'survey'])], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:46.490782Z",
     "start_time": "2018-01-26T18:06:46.486153Z"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "preprocessor.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "##### Methods of `preprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:46.929964Z",
     "start_time": "2018-01-26T18:06:46.925471Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method add_stopwords in module tm_preprocessor.preprocessor:\n",
      "\n",
      "add_stopwords(additional_stopwords) method of tm_preprocessor.preprocessor.Preprocessor instance\n",
      "    Add additional stopwords to the current `Preprocessor` object.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    additional_stopwords : iteratble object (list/tuple/numpy array...; init to None)\n",
      "        Additional stopwords.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessor.add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:47.154915Z",
     "start_time": "2018-01-26T18:06:47.149139Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method remove_stopwords in module tm_preprocessor.preprocessor:\n",
      "\n",
      "remove_stopwords() method of tm_preprocessor.preprocessor.Preprocessor instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessor.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:47.385764Z",
     "start_time": "2018-01-26T18:06:47.381775Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method normalize in module tm_preprocessor.preprocessor:\n",
      "\n",
      "normalize(normalizer, min_freq=0.05, max_freq=0.95, min_length=1) method of tm_preprocessor.preprocessor.Preprocessor instance\n",
      "    Normalize corpus by either lemmatization or stemming. Also remove rare/common and short words\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    stemmer : nltk.stem stemmers or lemmatizers\n",
      "        Stemmer/lemmatizer to use. See http://www.nltk.org/api/nltk.stem.html. If `None`, do not stem.\n",
      "    min_freq : float\n",
      "        The minimum frequency (in ratio) of a token to be kept\n",
      "    max_freq : float\n",
      "        The maximum frequency (in ratio) of a token to be kept\n",
      "    min_length : int\n",
      "        The minimum length of a token to be kept\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessor.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:06:47.632677Z",
     "start_time": "2018-01-26T18:06:47.628751Z"
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_word_ranking in module tm_preprocessor.preprocessor:\n",
      "\n",
      "get_word_ranking() method of tm_preprocessor.preprocessor.Preprocessor instance\n",
      "    Get the ranking of words (tokens). Note that this should be done a\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    pd.DataFrame\n",
      "        Sorted dataframe with columns `word` and corresponding `frequency`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessor.get_word_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuations and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:01.303404Z",
     "start_time": "2018-01-26T18:31:01.300546Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.remove_digits_punctuactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:02.092626Z",
     "start_time": "2018-01-26T18:31:02.084492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human',\n",
       "  'machine',\n",
       "  'interface',\n",
       "  'for',\n",
       "  'lab',\n",
       "  'abc',\n",
       "  'computer',\n",
       "  'applications'],\n",
       " ['a',\n",
       "  'survey',\n",
       "  'of',\n",
       "  'user',\n",
       "  'opinion',\n",
       "  'of',\n",
       "  'computer',\n",
       "  'system',\n",
       "  'response',\n",
       "  'time',\n",
       "  't'],\n",
       " ['he', 'went', 'to', 'the', 'gym', 'and', 'swam'],\n",
       " ['the', 'eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'and', 'human', 'system', 'engineering', 'testing', 'of', 'eps'],\n",
       " ['relation',\n",
       "  'of',\n",
       "  'user',\n",
       "  'perceived',\n",
       "  'response',\n",
       "  'time',\n",
       "  'to',\n",
       "  'error',\n",
       "  'measurement'],\n",
       " ['the', 'generation', 'of', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['the', 'intersection', 'graph', 'of', 'paths', 'in', 'trees'],\n",
       " ['graph',\n",
       "  'minors',\n",
       "  'iv',\n",
       "  'widths',\n",
       "  'of',\n",
       "  'trees',\n",
       "  'and',\n",
       "  'well',\n",
       "  'quasi',\n",
       "  'ordering'],\n",
       " ['graph', 'minors', 'a', 'survey']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:03.172805Z",
     "start_time": "2018-01-26T18:31:03.168277Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.remove_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:03.647494Z",
     "start_time": "2018-01-26T18:31:03.642375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['gym', 'swam'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `min_freq` to 0.1, `max_freq` to 0.9, and `min_length` to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:07.938785Z",
     "start_time": "2018-01-26T18:31:07.935581Z"
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 0\n",
    "max_freq = 1\n",
    "min_len = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:09.027882Z",
     "start_time": "2018-01-26T18:31:09.025215Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:09.462532Z",
     "start_time": "2018-01-26T18:31:09.458580Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:12.597820Z",
     "start_time": "2018-01-26T18:31:10.037211Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.normalize(lemmatizer, min_freq, max_freq, min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:31:14.185272Z",
     "start_time": "2018-01-26T18:31:14.179697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'application'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['gym', 'swam'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'test', 'eps'],\n",
       " ['relation', 'user', 'perceive', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'tree'],\n",
       " ['intersection', 'graph', 'path', 'tree'],\n",
       " ['graph', 'minor', 'iv', 'width', 'tree', 'quasi', 'order'],\n",
       " ['graph', 'minor', 'survey']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serialize the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save as `Market Matrix format`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-22T16:11:52.935190Z",
     "start_time": "2017-07-22T16:11:52.929845Z"
    }
   },
   "source": [
    "See http://math.nist.gov/MatrixMarket/formats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:32:45.867769Z",
     "start_time": "2018-01-26T18:32:45.859096Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.serialize(format_='MmCorpus', path='~/Desktop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies of words/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-26T18:33:08.222964Z",
     "start_time": "2018-01-26T18:33:08.191861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>system</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tree</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graph</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eps</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>response</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>survey</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>computer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>interface</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>iv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>measurement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>path</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>width</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quasi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>intersection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>unordered</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>random</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>generation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>error</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>perceive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>relation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>machine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>engineering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>management</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>swam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gym</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>opinion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>application</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>abc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>lab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>order</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  frequency\n",
       "0         system          4\n",
       "1           user          3\n",
       "2           tree          3\n",
       "3          graph          3\n",
       "4          human          2\n",
       "5            eps          2\n",
       "6           time          2\n",
       "7          minor          2\n",
       "8       response          2\n",
       "9         survey          2\n",
       "10      computer          2\n",
       "11     interface          2\n",
       "12            iv          1\n",
       "13   measurement          1\n",
       "14          path          1\n",
       "15         width          1\n",
       "16         quasi          1\n",
       "17  intersection          1\n",
       "18     unordered          1\n",
       "19        binary          1\n",
       "20        random          1\n",
       "21    generation          1\n",
       "22          test          1\n",
       "23         error          1\n",
       "24      perceive          1\n",
       "25      relation          1\n",
       "26       machine          1\n",
       "27   engineering          1\n",
       "28    management          1\n",
       "29          swam          1\n",
       "30           gym          1\n",
       "31       opinion          1\n",
       "32   application          1\n",
       "33           abc          1\n",
       "34           lab          1\n",
       "35         order          1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.get_word_ranking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "331px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "863px",
    "left": "0px",
    "right": "1468px",
    "top": "113px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
